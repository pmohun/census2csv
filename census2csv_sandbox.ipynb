{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! python3\n",
    "# census2csv\n",
    "\n",
    "# This script is used to scrape and consolidate demographic, population, and housing data \n",
    "# from the United States census website at: https://www.ffiec.gov/census/default.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from tkinter import filedialog\n",
    "from nested_dict import nested_dict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import specific county codes for parsing\n",
    "\n",
    "geo_info = ([36047,36061,6075,11001,6037,17031,25025,36081,6081,25021,36005,6111,36085,36119,\n",
    "            24033,53025,6059,34017,25017,6071,24031,6029,36059,42007,6065,6083,50003,6085,17197])\n",
    "\n",
    "# standardize for use in parsing\n",
    "state = []\n",
    "county = []\n",
    "for geo in geo_info:\n",
    "    geo = str(geo)\n",
    "    if len(geo) > 7:\n",
    "        geo = geo[:-1].replace(\".\",\"\") # remove the last digit, and remove decimals\n",
    "    if len(geo) < 5:\n",
    "        geo = '0' + geo\n",
    "    state.append(geo[:2])\n",
    "    county.append(geo[2:])\n",
    "\n",
    "# define number of pages\n",
    "\n",
    "# define columns for tables\n",
    "columns = []\n",
    "demographic_columns = ['Tract Code','Tract Income Level','Distressed (Y/N)','Tract Median Fam Income','MSA Median Fam Income','2017 Median Fam Income','2015 Median Fam Income','Tract Population','Tract Minority %','Minority Pop','Owner Occupied Units','1-4 Fam Units','state','county']\n",
    "population_columns = ['Tract Code','Tract Population','Tract Minority %','Number of Families','Number Households','Non Hispanic White Pop','tract_minority_pop','american_indian_pop','asian_islander_pop','black_population','hisplanic_population','mixed_population','state','county']\n",
    "housing_columns = ['Tract Code','Total Housing Units','1-4 Family Units','Median House Age','Inside City?','Owner Occupied Units','Vacant Units','Owner Occupied 1-4 Fam Units','Renter Occupied','state','county']\n",
    "columns.append(demographic_columns)\n",
    "columns.append(population_columns)\n",
    "columns.append(housing_columns)\n",
    "\n",
    "report = ['demographic','population','housing'] # type of data to pull from census site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Page(url):\n",
    "    # This function is used to pull the max page number from each counties report on the FFIEC website\n",
    "    # Example of url for this function is: https://www.ffiec.gov/census/report.aspx?year=2017&county=019&tract=ALL&state=06&report=demographic\n",
    "    html_response = requests.get(url).text.encode('utf-8') # request html (hot garbage)\n",
    "    soup = BeautifulSoup(html_response, 'html.parser')\n",
    "    pages = soup.find_all(\"span\", class_=\"main-body\") # page number sits in this class\n",
    "    pages = str(pages[4].string) # specific to this site, lists page n of n\n",
    "    try:\n",
    "        page = int(pages[-2:]) # hoping this is a common format\n",
    "    except Exception:\n",
    "        page = 1\n",
    "    return page\n",
    "\n",
    "def consolidate_pages(state):\n",
    "    pages = []\n",
    "    for i in range(len(state)):\n",
    "            # This uses the demographic report to pull the max page number, although any type of report should give the same result\n",
    "            base_url = 'https://www.ffiec.gov/census/report.aspx?year=2017&state=' + state[i] + '&msa=&county=' + county[i] + '&tract=ALL&report=demographic'\n",
    "            print(base_url)\n",
    "            try:\n",
    "                page = get_Page(base_url) # iterate through each state/county pair and pull the max page number\n",
    "            except Exception: # if there is only one page, this exception will be thrown\n",
    "                page = 1\n",
    "            print(page)\n",
    "            page = str(page)\n",
    "            pages.append(page)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.034482758620689655\n",
      "0.06896551724137931\n",
      "0.10344827586206896\n",
      "0.13793103448275862\n",
      "0.1724137931034483\n",
      "0.20689655172413793\n",
      "0.2413793103448276\n",
      "0.27586206896551724\n",
      "0.3103448275862069\n",
      "0.3448275862068966\n",
      "0.3793103448275862\n",
      "0.41379310344827586\n",
      "0.4482758620689655\n",
      "0.4827586206896552\n",
      "0.5172413793103449\n",
      "0.5517241379310345\n",
      "0.5862068965517241\n",
      "0.6206896551724138\n",
      "0.6551724137931034\n",
      "0.6896551724137931\n",
      "0.7241379310344828\n",
      "0.7586206896551724\n",
      "0.7931034482758621\n",
      "0.8275862068965517\n",
      "0.8620689655172413\n",
      "0.896551724137931\n",
      "0.9310344827586207\n",
      "0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "report = ['demographic','population','housing']\n",
    "baseurl = 'https://www.ffiec.gov/census/report.aspx?year=2017&county=019&tract=ALL&state=06&report=demographic'\n",
    "\n",
    "for j in range(len(report)): \n",
    "    column_df = columns[j] # use the correct headers for each type of census report\n",
    "    df = pd.DataFrame(columns = columns[j]) # initialize dataframe to hold consolidated reports\n",
    "    for i in range(len(state)):\n",
    "        print((i/((len(state)*len(report))))) # progress bar so I don't get impatient\n",
    "        url = 'https://www.ffiec.gov/census/report.aspx?year=2017&state=' + state[i] + '&msa=&county=' + county[i] + '&tract=ALL&report=' + report[j]+ '&page=' + pages[i]\n",
    "        html = requests.get(url).content\n",
    "        df_list = pd.read_html(html)\n",
    "        df_list = df_list[-1] # transpose to meet df structure\n",
    "        df_list['state'] = state[i]\n",
    "        df_list['county'] = county[i]\n",
    "        df_list = df_list.iloc[1:,:] # remove first row to prevent duplicate column headers\n",
    "        df_list.columns = column_df\n",
    "        df = df.append(df_list) # add to data frame\n",
    "        df.to_csv('censusInfo_' + report[j] + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "base_url = 'https://www.ffiec.gov/census/report.aspx?year=2017&state=' + state[i] + '&msa=&county=' + county[i] + '&tract=ALL&report=' + report[j]\n",
    "#get_Page(url):\n",
    "html_response = requests.get(base_url).text.encode('utf-8') # request html (hot garbage)\n",
    "soup = BeautifulSoup(html_response, 'html.parser')\n",
    "pages = soup.find_all(\"span\", class_=\"main-body\") # page number sits in this class\n",
    "#table = soup.find_all('table')[0]\n",
    "#elements = table.find_all(\"a\")\n",
    "#elements[0].a['href']\n",
    "#elements.find_all(\"page\")\n",
    "pages = str(pages[4].string) # specific to this site, lists page n of n\n",
    "#pages = (pages[4].contents)\n",
    "print(page)\n",
    "page = (pages[-2:]) # hoping this is a common format\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in state:\n",
    "    dict[st] = {}\n",
    "    for ct in county:\n",
    "        url = 'https://www.ffiec.gov/census/report.aspx?year=2017&state=' + state[i] + '&msa=&county=' + county[i] + '&tract=ALL&report=demographic&page=1'\n",
    "        html_response = requests.get(url).text.encode('utf-8') # request html (hot garbage)\n",
    "        soup = BeautifulSoup(html_response, 'html.parser')\n",
    "        pages = soup.find_all(\"span\", class_=\"main-body\") # page number sits in this class\n",
    "        pages = str(pages[4].string) # specific to this site, lists page n of n\n",
    "        page = int(pages[-2:]) # hoping this is a common format\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "page = '1'\n",
    "for st in state:\n",
    "    dict[st] = {}\n",
    "    for ct in county:\n",
    "        dict[st][ct] = {page}\n",
    "\n",
    "for key,value in dict.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['36', '36', '06', '11', '06', '17', '25', '36', '06', '25', '36', '06', '36', '36', '24', '53', '06', '34', '25', '06', '24', '06', '36', '42', '06', '06', '50', '06', '17'] ['047', '061', '075', '001', '037', '031', '025', '081', '081', '021', '005', '111', '085', '119', '033', '025', '059', '017', '017', '071', '031', '029', '059', '007', '065', '083', '003', '085', '197']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'06': ['075', '037', '081', '111', '059', '071', '029', '065', '083', '085'],\n",
       " '11': ['001'],\n",
       " '17': ['031', '197'],\n",
       " '24': ['033', '031'],\n",
       " '25': ['025', '021', '017'],\n",
       " '34': ['017'],\n",
       " '36': ['047', '061', '081', '005', '085', '119', '059'],\n",
       " '42': ['007'],\n",
       " '50': ['003'],\n",
       " '53': ['025']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dict = {}\n",
    "#for st in state:\n",
    "#    dict[st] = {}\n",
    "print(state, county)   \n",
    "dict = {}\n",
    "for i, j in zip(state, county):\n",
    "    dict.setdefault(i, []).append(j)\n",
    "\n",
    "dict\n",
    "\n",
    "#dict['06']\n",
    "#page = '1'\n",
    "#for key,value in dict.items():\n",
    "#    dict[value].append(nested)\n",
    "#    for countyname in value:\n",
    "#        nested = {countyname: page}\n",
    "        \n",
    "#dict'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
